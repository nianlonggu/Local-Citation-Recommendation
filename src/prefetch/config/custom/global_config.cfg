{
    ## These are the parameters you need to specify when you want to train the model on your custom dataset
    "prefetch_model_folder":"../../model/prefetch/custom/",
    "paper_database_path":"../../data/custom/papers.json",
    "context_database_path":"../../data/custom/contexts.json",
    "train_corpus_path":"../../data/custom/train_with_prefetched_ids.json",
    "log_folder":"../../log/prefetch/custom/",
    "prefetch_embedding_path":"../../embedding/prefetch/custom/paper_embedding.pkl",
    "input_corpus_path_for_get_prefetched_ids_during_training":"../../data/custom/train.json",
    "output_corpus_path_for_get_prefetched_ids_during_training":"../../data/custom/train_with_prefetched_ids.json",
    "input_corpus_path_for_validation":"../../data/custom/val.json",
    "output_corpus_path_for_validation_with_prefetched_ids":"../../data/custom/val_with_prefetched_ids.json",
    "input_corpus_path_for_test":"../../data/custom/test.json",
    "max_num_samples_per_batch":100, # max number of queries/documents within one batch. This depends on how many GPU memory in total for all GPUs. typically set to 100 if you have 1 GPU (11 GB). 
    "n_device":1, # number of GPUs available
    "max_num_loops_for_training_updating_embedding_and_prefetched_ids":2, # This will determine when the training loop will stop automatically. For large dataset this value should be large enough (e.g. 100), For small dataset like ACL-200, 2 loops will produce good enough results. 
    "num_training_examples_with_prefetched_ids_for_reranking":10000, # How many training examples for which we get the final prefetched ids, and use them to train the reranking system. Note that we may not need all the training examples to fine-tune the reranker. If the value is 0, this means we get the prefetehced ids for all training examples, and use them to train the reranker. Here we use 10000 as an example
    "num_val_examples_with_prefetched_ids_for_reranking":1000, # How many training examples for which we get the final prefetched ids, and use them to train the reranking system.

    
    
    ## These are the parameters that are ussually kept unchanged 
    "print_every":500,  ## to report the training loss every certain iterations
    "save_every":5000,  ## save the checkpoint every certain iterations
    "max_num_iterations":5000,  ## In one train -> update embedding -> update prefetched ids loop, the total number of training iterations in the training phase, set it to a larger value (e.g. 10000) on large dataset, such as arXiv
    "K_list":[10,20,50,100,200,500,1000,2000], # This is the top Ks we are interested in.
    "top_K_prefetched_ids_for_mining_hard_negative":100,
    "top_K_prefetched_ids_for_reranking":2000, ## we first prefetch 2000 candidates then rerank them with the reranker
    "num_papers_with_updated_embeddings_per_loop":0,  ## When the paper database is large, such as contains millions of papers, we can just re-compute part of the paper database, and use this partial updated embedding index to prefetch papers, and mine hard negative from them, value 0 means update all papers embeddings
    "num_training_examples_with_updated_prefetched_ids_per_loop":50000, ## When the training set is very large, we can just update the prefetched ids for part of the training examples that are randomly sampled from the whole training dataset, and use them to train the text encoder in the next loop
    "num_val_examples_per_loop":0,  ## We can evaluate the val performance using only part of the validation set. The purpose of the validation is only to provide a rough information about the prefetching performance in an efficient way. value 0 means use the whole validation set to evaluate the performance
    "unigram_words_path":"../../model/glove/vocabulary_200dim.pkl",
    "unigram_embedding_path":"../../model/glove/unigram_embeddings_200dim.pkl",
    "train_log_file_name":"train.log",
    "val_log_file_name":"validate_NN.log",
    "test_log_file_name":"test_NN.log",
    "max_n_positive": 1,  ## for each query, we get 1 positive paper (the paper cited by the query), 3 prefetched but not cited paper and 1 randomly sampled paper. We keep add more queies into a batch, until we reach the max_num_samples_per_batch
    "max_n_hard_negative": 3,
    "max_n_easy_negative": 1,
    "num_workers": 2,
    "initial_learning_rate": 1e-4,
    "l2_weight": 1e-5,
    "dropout_rate": 0.1,
    "moving_average_decay": 0.999,
    "base_margin": 0.05,
    "similarity": "cosine",
    "positive_irrelevance_levels":[1,2],
    "max_num_checkpoints": 20,
    ## The following part is related to the parameters of the text encoder
    "embed_dim": 200,  
    "num_heads": 8,
    "hidden_dim": 1024,
    "max_seq_len": 512,
    "max_doc_len": 3,
    "n_para_types": 100,
    "num_enc_layers": 1,
    "document_title_label": 0,
    "document_abstract_label": 1,
    "document_fullbody_label": 2,
    "citation_title_label": 0,
    "citation_abstract_label": 1,
    "citation_fullbody_label": 2,
    "citation_context_label": 3,
    "padding_paragraph_label": 10
}
